スケジュラーは、ご存じだと思いますが、執行するタスクを選別する、
OS の中で最も重要で複雑なコンポーネントの一つと言われていますが、
いろんな理由があります。

一つ目は、平行プログラミングのツールの一つ、同期プリミティブに強く依存するからです。
LinuxカーネルではRCUに依存しますし、タスクの切り替え中に割り込みなどの機能も使います。

次は、ほかのサブシステムとの交互作用が多いからです。例としては、メモリマネジメントを司るシステムがあります。
　　　　　　　　　　　        （こうごさよう）　　　　　　　　　　　　　　　                       （つかさどる）
最後はそのシステムの大きさです。Linuxカーネルでは、スケジュラーに関連するファイル数は６０個以上であり、
併せて２７０００ライン以上のコードで成り立っています。

つまり、スケジューラは多くのシステム指標に影響を与えるため、
スケジューラにある問題を解決することが今でも大切な課題です。

＝＝＝＝
換頁!
＝＝＝＝

そのスケジュラーの問題の一つは、ワークロードに対応しきれないところです。


考えるまでもないんですが、
自宅においてある PC 上に動いているタスクは、
クラウドサーバー上のタスクの特徴と絶対に異なるし、その特徴が変わることもあります。
そういうあらゆる特徴に対応できるスケジュラを開発するのは、
難しいというか、不可能だと思われています。

たとえば、Linuxカーネルのデフォルト、CFSスケジュラーですが、デフォルトにされてるだけあって、いい表現があるはずなんですが、
クラウド環境において、同じノード上に数千個コンテイナーを同時に動かすことになると、
約8パーセントのＣＰＵサイクルが、スケジュリングのためだけに使われちゃいます。

この状況に対応するため、ワークロードに特化したのスケジュラーが多く提出されています。例としてはこの二つです。

しかし、OS の運行中にスケジュラーを変えること自体が、大きな資源がかかります。

＝＝＝＝
換頁!
＝＝＝＝

上に見えるのは、伝統的にOSのアップデートするためのステップです。
少なくとも、再起動からアプリケーション執行が再び始まるまでがダウンタイムになります。

これを高速化するため、ＶＭマイグレーション　という
更新されたＯＳを別の仮想マシンで起動・初期化を終えてから、
旧システムからタスクなどの情報をネットワークを通してで新ＯＳに移行する　手法が提出されていますが、
これも問題があります。

このマイグレーションにかかる時間は、旧ＯＳ上のスレッド数に連れて上昇しますが、
今現在のサーバー環境では、一万個以上のスレッドが存在していても珍しくなく、
移行するためにも数分かかります。

しかし、金融取引など、レイテンシーに敏感なアプリケーションは、
わずかな停止時間しか許容できませんので、
停止時間を数十ミリ秒レベルにまで減らすことができるスケジューラの更新方法が
望まれています。

＝＝＝＝
換頁! 
＝＝＝＝

ここで、ライブアップデートに関する先行研究をいくつかを紹介します。
左にある図は、代表的な先行研究と、それらの Generality と Expressiveness を表すプロットです。

Y軸の Generality は、システムを適応するための、カーネル制限の多さを指します。
上に行けば行くほど、制限が少ないです。
たとえば、ＰＲＯＴＥＯＳは、マイクロカーネルというタイプのカーネルにしか適用できず、普通なLinuxサーバーでは全く使えませんので、
Ｇｅｎｅｒａｌｉｔｙが低いです。

それに対して、X軸の Expressiveness は、アップデートできるシステムの割合を指します。
右に行けば行くほど、変更する割合が大きいです。たとえば、ＶＭ－ＰＨＵは、ＶＭ上のゲストにしか使えませんが、
ＯＳほとんど全体の変更ができるため、Expressiveness が高いです。

そして、スケジュラライブアップデート界隈で有名なのはこの ghOSt です。このシステムは、
スケジュラーのアップデートを、ユーザースペースですることを可能にしました。便利さは抜群ですが、これもいくつかの問題をもたらします。
まず、これを達成するため、サーバー側のカーネルコードを変更する必要がありますし、スケジュラーの一部しか変更できないし、
カーネルスペースとユーザースペース間のコミュニケーションも必要とするため、大きなオーバーヘッドをもたらすこともあります。

今回の論文で提出されるシステム、PLUGSCHED は、このプロットの真ん中の右上寄りになります。

＝＝＝＝
換頁!
＝＝＝＝

そして、PLUGSCHEDのシステムデザインを説明する前に、達成したい四つのゴールを簡単にまとめます。

まずは、適切なExpressiveness　です。詳しくは、
コンポーネントレベルのライブアップデートの対応をしたいです。
また、ロールバックも支援するため、データの状態も移行可能であるべきということを指します。

次に、高いＧｅｎｅｒａｌＩｔｙです。
このシステムを用いるためには、Linuxカーネルへ制限も、カーネルコードへの変更もなしで適用することを指します。

さらに、短い停止時間と安全性の両立を目指します。
停止時間の削減はこのシステムの主なゴールです。
ここの「安全性」という言葉は、システムセキュリティの概念とは違って、
アップデートするとき、システムのほかの部分に変更を与えないことを指します。

最後に、使いやすさ重視、のところです。
開発者が提出するシステムでパッチ、もしくは新しいスケジュラーを開発に集中でき、
アップグレードやロールバックは、モジュールのインストール/アンインストールのみによって行えることを指します。

＝＝＝＝
換頁!
＝＝＝＝

そして、ＰＬＵＧＳＣＨＥＤのオーバービューです。
ＰＬＵＧＳＣＨＥＤは、主に三つの部分に分けられて、さらに８のステップに分けられます。

一つ目のステップは、スケジュラーのモジュラー化です。ここで、スケジュラに関連するコードを抽出し、次の開発の部分に提供します。

二つ目の開発ステップは、抽出されたコードを用いて変更を実装して、ｒｐｍ形式でのモジュールを作成します。
このステップは、論文の中でも紹介されていませんが、このシステムを用いる開発者により内容が大きく変わるためだと思っています。

三つめは、デプロイメントの部分です。作られたモジュールをインストール・アンインストールにより、ＯＳアップデートを適用します。
この緑色の背景に囲まれている段階は、カーネルを内部で実行され、マシンを停止する必要もあります。つまり、この部分がダウンタイムです。

＝＝＝＝
換頁!
＝＝＝＝

これから、ステップを沿って紹介します。まずは、スケジュラーのモジュラー化です。

Ｌｉｎｕｘカーネルのコードは、コンポーネント各々のディレクトリがあります。ただし、今回は、直接そのディレクトリ丸ごとぱくってはいけません。
その理由は、その中のすべての関数とデータがスケジュラーに関連するではなく、それらを消すことはカーネルコードの変更する必要になります。
この問題を解決するため、ＰＬＵＧＳＣＨＥＤは関数目線でスケジュラーの境界線を探ります。

データは、内部と外部の二つのタイプに分けます。
内部データは、スケジュラーにしか使われません。そして外部データは、カーネル全般に使われるます。

それに対して、関数は３つのタイプに分けます。内部、外部に加えて、さらにインターフェイス関数があります。
内部関数は、スケジュラー内部のことを指します。内部関数は、内部関数をしか呼ばないし、内部とインターフェイス関数にしか呼ばれません。
インターフェイス関数は、外部関数によばれ、内部関数を呼びます。
外部関数は、直接内部関数を呼べないうえ、ほかの外部関数にしか呼ばれません。

そして、スケジュラーに関する関数は、この内部とインターフェイス関数の集合です。

＝＝＝＝
換頁!
＝＝＝＝

そして、こちらが関数を分別するためのアルゴリズムです。

まずは、インプットとしてインターフェイス関数と全関数のリストを入力します。

さらに、カーネルコールグラフと言い、関数の呼び合う関係を表すグラフを、ＧＣＣプラグインから取得します。
そして、ライン６から１７のメインループに入ります。

下のこのグラフは結果を表しますが、最初は緑のノード、つまりインターフェイス関数しかわかりません。
メインループの役割は、現在の情報により推定できる外部関数を探します。このグラフの青色のものです。
このループをなんかいも執行することで、すべての外部関数がわかり、残った関数を内部関数とします。

＝＝＝＝
換頁!
＝＝＝＝

最後に、コードの抽出です。
コードを、事前に指定できるパスに抽出しますが、外部関数は開発者による変更をしてはいけないため、少し手を組みます。

外部関数は、関数の中身、つまりブレースに囲まれる部分を削除し、後ろにセミコロン一つつけて、関数宣告にします。
また、外部データをＧＣＣプラグインのこれを参照し、同じく宣告にします。

Ｌｉｎｕｘバージョン 4.19 での試行結果は、このテーブルの通りです。

＝＝＝＝
換頁!
＝＝＝＝

そして、アップデート開発を省いて、ライブアップデートの部分に移ります。
最初のステップは、スタックインスペクションです。

まず、すべてが始まるまえに、stop_machine() 関数を使い、システムをいったん停止します。ここからがダウンタイムです。
そして、kpatch という先行研究同じく、コールスタックをかくにんして、再びシステムを動かせると、旧システムのテキストを執行するスレッドがないことをかくにんします。
ただし、kpatch と比べると、検査するリストは大きいため、マルチコアでこのステップを行います。さらに、リストのなかでのサーチは、バイナリサーチを用います。
これらの改善策により、スタックインスペクションの計算量はＴかけるＤかけるＮから、ＴわるＣかけるＤかける logＮになります。

＝＝＝＝
換頁!
＝＝＝＝

次は、関数のリダイレクトです。これは意外とシンプルですが、
旧関数の一つ目のインストラクションを、新しい関数へ移行するジャンプ指令に書き換えるのみです。

ただし、__schedule() という、一つ例外があります。
この関数が呼ばれると、スケジュラーはすけじゅリングをして、コンテクストスイッチ関数を呼び、執行するタスクを切り替えますが。
スケジュラーの指令を書き換えるも、睡眠状態のタスクは起こされると、古いコンテクストスイッチに戻ります。

これを解決するためには、スケジュラー関数の重要な仕事はすべてコンテクストスイッチ関数の前に終えるという性質を使います。
新しいスケジュラー関数は、コンテクストスイッチ関数を呼ぶことになると、古いコンテクストスイッチ関数を呼びます。こうして混ぜることを回避します。

＝＝＝＝
換頁!
＝＝＝＝

最後に、データの移行を行います。

多くのデータはステートがあり、アップデート前後の状態を保持すべきです。
例えば、ランニングステートのタスクは移行後をランニングステートのままである必要があります。

これを行うため、データタイプのさらなる分類が必要です。
外部データは、元々へんこうが許されないので、直接移行が可能ですが、
内部データは、メモリ取得時間と再構築の必要性により、プライベート、シェアド、クリティカル、ノンクリティカルのクラスに分けます。
クリティカルデータであれば、再構築が必要です。ノンクリティカルであれば、プライベートデータは再初期化で、共有データならば直接古いデータを使います。

再構築の原因を論文で述べられていませんが、クリティカルデータリストっていうリストがあって、そこにある構造体を調べると、
共有するのはリンクリストの一部であることです。ポインタ操作があり、アドレスの変更もお絶対に起こすため再構築が必要になったのではないかと推測しました。

これを終えると、やっとダウンタイムを終えて、システム更新されてアプリの執行を続けられます。

＝＝＝＝
換頁!（実験開始）
＝＝＝＝

これからは、このシステムをつかった評価について話します。
使われているハードウェアや実装の情報はこの通りです。　　
このシステムはオープンソースですので、詳しい実装情報はここで見れます。

＝＝＝＝
換頁!
＝＝＝＝

まずは、違うアップデートケースとダウンタイムとの関連です。

スケジュラーへのアップデートは、主に３種類があります。
パッチは、バグ修復など、数行～数十行レベルの変更を指します。
フィーチャーは、現行のスケジュラに新しく機能の追加などを指します。
最後に、スケジュラー全体の書き換えがあります。

右側のテーブルから、この３種類のアップデートのライン数の違いがあることが判ります。
ただし、右下のグラフを見ると、これらのモジュールのインストールもアンインストールるも、
ダウンタイムに顕著な影響がみられません。

＝＝＝＝
換頁!
＝＝＝＝

次に、ダウンタイムの分析です。
まずは、シングルコアで、ワークロードがない実験用機器にて測定すると、２ミリ秒しかダウンタイムがありません。
　　　　　　　　　　　　　　　　　　　　　　（そくてい）

さらに、アリババ社に実際の４００個の商用サーバーで実験を行うと、マルチコアの場合、
ほとんどのダウンタイムは１ミリ秒以下になります。

つまりこれは、研究背景で要求されたダウンタイムに満足していることを指します。

注目するのは、実行時間はシステムデザインのおかけで、９8％が初期化するための時間です。
一つ意見があるのは、単位を統一してほしいところです。

＝＝＝＝
換頁!
＝＝＝＝

そして、ＰＬＵＧＳＣＨＥＤのスケーラビリティの測定です。
実験によると、マルチコアでのスタックインスペクションは、２４コアの場合で最もいい結果が出ますが、
６コアからはほとんど変わりません。

この優れていないスケーラビリティは、２つの理由があると推測　　されています。

一つ目は、ワークロードがバランスでないことです。
そして２つ目は、スタックインスペクションを開始するには数ミリ秒かかるため、さらに多いコアの効能向上をなくしちゃいます。

＝＝＝＝
換頁!
＝＝＝＝

最後に、並行スタックインスペクションとデーター再構築です。
これらの実験は、６コアでテストしています。

結果によると、スタックインスペクションは、シングルコアもマルチコアも、動いているスレッドに連れた執行時間がリニアタイムです。
それに対して、データの’再構築は、マルチコアの場合、動いているスレッド数が変わるも同じ時間をかかることが観測されました。

＝＝＝＝
換頁!
＝＝＝＝

最後にまとめです。

ワークロード特化型のスケジュラがありますが、スケジュラをワークロードに応じて変更することが困難ですので、
本論文ではそういうライブアップデートが短いダウンタイムでできるシステムを提出しました。

このシステムは、コードを抽出して、開発者に提供して、
開発者がモジュールを作成してカーネルにインストールすることでアップデートができます。

実験により、システムが有効であることと、マルチコアでの効能向上も観測されます。
フューチャーワークですが、同じ手法をほかのシステム変更に応用することが述べられています。